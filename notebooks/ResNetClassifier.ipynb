{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c862eb4",
   "metadata": {},
   "source": [
    "Call 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f20c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python: C:\\Users\\chunn\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "print(\"Using Python:\", sys.executable)\n",
    "\n",
    "# 1) Clean out CPU-only builds\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\",\n",
    "                       \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "\n",
    "# 2) Install CUDA-enabled wheels into THIS env\n",
    "# If cu124 is unavailable for you, change cu124 -> cu126 (or any cu12x shown on pytorch.org)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                       \"--force-reinstall\", \"--no-cache-dir\",\n",
    "                       \"--index-url\", \"https://download.pytorch.org/whl/cu124\",\n",
    "                       \"torch\", \"torchvision\", \"torchaudio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7314f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, math, os\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    confusion_matrix, matthews_corrcoef\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fb8dc",
   "metadata": {},
   "source": [
    "Cell 2: Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eacdb7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATASET_ROOT: ../dataset2\n",
      "CWD: c:\\Users\\Max\\Desktop\\ComputerVision\\INF3001_Project\\notebooks\n",
      "Looking for: C:\\Users\\Max\\Desktop\\ComputerVision\\INF3001_Project\\notebooks\\dataset2\\images\n",
      "Exists? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\i'\n",
      "C:\\Users\\Max\\AppData\\Local\\Temp\\ipykernel_16096\\2971055395.py:6: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  TRAIN_DIR       = f\"dataset2\\images\"   # main folder (we’ll split this virtually)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Configurations\n",
    "# -------------------------\n",
    "\n",
    "DATASET_ROOT    = \"../dataset2\"       # <-- changed to dataset2\n",
    "TRAIN_DIR       = f\"dataset2\\images\"   # main folder (we’ll split this virtually)\n",
    "CHECKPOINT_PATH = \"augment_classifier_best.pth\"\n",
    "CLASS_MAP_JSON  = \"class_mapping.json\"\n",
    "\n",
    "# ====Hyperparameters ====\n",
    "IMAGE_SIZE     = 224\n",
    "BATCH_SIZE     = 32\n",
    "EPOCHS         = 15\n",
    "LEARNING_RATE  = 1e-4\n",
    "WEIGHT_DECAY   = 1e-3\n",
    "NUM_WORKERS    = 2\n",
    "USE_AMP        = True\n",
    "VAL_RATIO      = 0.15     # 15% validation from train\n",
    "TEST_RATIO     = 0.15     # 15% test from train\n",
    "SEED           = 42\n",
    "MOMENTUM       = 0.9\n",
    "\n",
    "print(\"Using DATASET_ROOT:\", DATASET_ROOT)\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Looking for:\", Path(TRAIN_DIR).resolve())\n",
    "print(\"Exists?\", Path(TRAIN_DIR).is_dir())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7735d3",
   "metadata": {},
   "source": [
    "Cell 3: Parse Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e14a8e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotations: C:\\Users\\Max\\Desktop\\ComputerVision\\INF3001_Project\\dataset2\\annotations.json\n",
      "Using images dir : C:\\Users\\Max\\Desktop\\ComputerVision\\INF3001_Project\\dataset2\\images\n",
      "\n",
      "Detected 7 classes: ['Boots', 'Ear Protection', 'Gloves', 'Goggles', 'Helmet', 'Mask', 'Vest']\n",
      "Total images listed in annotations: 1391\n",
      "Total images FOUND on disk         : 1390\n",
      "WARNING: 1 image paths from annotations not found under ..\\dataset2\\images\n",
      "\n",
      "Instances per class:\n",
      "                 Boots: 506\n",
      "        Ear Protection: 43\n",
      "                Gloves: 826\n",
      "               Goggles: 443\n",
      "                Helmet: 1113\n",
      "                  Mask: 604\n",
      "                  Vest: 996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Updated for your dataset2 layout ===\n",
    "DATASET_ROOT = Path(\"../dataset2\")\n",
    "ANN_PATH     = DATASET_ROOT / \"annotations.json\"\n",
    "IMAGES_DIR   = DATASET_ROOT / \"images\"\n",
    "\n",
    "# Safety check\n",
    "if not ANN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"annotations.json not found at {ANN_PATH}\")\n",
    "if not IMAGES_DIR.exists():\n",
    "    raise FileNotFoundError(f\"images folder not found at {IMAGES_DIR}\")\n",
    "\n",
    "print(\"Using annotations:\", ANN_PATH.resolve())\n",
    "print(\"Using images dir :\", IMAGES_DIR.resolve())\n",
    "\n",
    "# === Parse annotations ===\n",
    "data = json.loads(ANN_PATH.read_text())   # format: { \"file.jpg\": [\"Helmet\",\"Vest\", ...], ... }\n",
    "\n",
    "# collect all class names\n",
    "all_labels = set()\n",
    "for lbls in data.values():\n",
    "    for l in lbls:\n",
    "        all_labels.add(str(l))\n",
    "class_names = sorted(all_labels)\n",
    "name2idx = {n:i for i,n in enumerate(class_names)}\n",
    "idx2name = {i:n for n,i in name2idx.items()}\n",
    "\n",
    "# build samples: (image_path, [class_name,...])\n",
    "samples = []\n",
    "missing = []\n",
    "for fname, lbls in data.items():\n",
    "    p = IMAGES_DIR / fname\n",
    "    if p.exists():\n",
    "        labs = sorted({str(l) for l in lbls})\n",
    "        samples.append((p, labs))\n",
    "    else:\n",
    "        missing.append(str(p))\n",
    "\n",
    "print(f\"\\nDetected {len(class_names)} classes: {class_names}\")\n",
    "print(\"Total images listed in annotations:\", len(data))\n",
    "print(\"Total images FOUND on disk         :\", len(samples))\n",
    "if missing:\n",
    "    print(f\"WARNING: {len(missing)} image paths from annotations not found under {IMAGES_DIR}\")\n",
    "\n",
    "# count instances per class (multi-label: each image contributes to all its labels)\n",
    "counts = Counter()\n",
    "for _, labs in samples:\n",
    "    counts.update(labs)\n",
    "print(\"\\nInstances per class:\")\n",
    "for c in class_names:\n",
    "    print(f\"  {c:>20s}: {counts.get(c,0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651cb05",
   "metadata": {},
   "source": [
    "Cell 4: Split Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c6fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Split Summary ===\n",
      "Total samples: 1390\n",
      "Train: 972  (69.9%)\n",
      "Val:   209    (15.0%)\n",
      "Test:  209   (15.0%)\n",
      "\n",
      "Train distribution:\n",
      "                Gloves: 353\n",
      "                 Boots: 354\n",
      "               Goggles: 59\n",
      "                Helmet: 143\n",
      "                  Vest: 12\n",
      "                  Mask: 27\n",
      "        Ear Protection: 24\n",
      "\n",
      "Val distribution:\n",
      "                Helmet: 31\n",
      "                 Boots: 76\n",
      "                  Mask: 6\n",
      "                Gloves: 76\n",
      "        Ear Protection: 5\n",
      "               Goggles: 13\n",
      "                  Vest: 2\n",
      "\n",
      "Test distribution:\n",
      "                Gloves: 76\n",
      "                 Boots: 76\n",
      "                Helmet: 31\n",
      "               Goggles: 13\n",
      "                  Mask: 6\n",
      "        Ear Protection: 5\n",
      "                  Vest: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def primary_label(labels):\n",
    "    # choose one label per image for stratification proxy\n",
    "    # here: first label alphabetically; you could use majority/frequency if you track it\n",
    "    return sorted(labels)[0] if labels else None\n",
    "\n",
    "X = np.arange(len(samples))\n",
    "y_proxy = np.array([name2idx[primary_label(lbls)] for _, lbls in samples])\n",
    "\n",
    "# test split first\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_RATIO, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y_proxy))\n",
    "\n",
    "# then val from remaining\n",
    "val_rel = VAL_RATIO / (1.0 - TEST_RATIO)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(trainval_idx, y_proxy[trainval_idx]))\n",
    "\n",
    "train_items = [samples[i] for i in trainval_idx[train_idx]]\n",
    "val_items   = [samples[i] for i in trainval_idx[val_idx]]\n",
    "test_items  = [samples[i] for i in test_idx]\n",
    "\n",
    "# --- After your train/val/test split code ---\n",
    "print(\"\\n=== Split Summary ===\")\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "print(f\"Train: {len(train_items)}  ({len(train_items)/len(samples)*100:.1f}%)\")\n",
    "print(f\"Val:   {len(val_items)}    ({len(val_items)/len(samples)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(test_items)}   ({len(test_items)/len(samples)*100:.1f}%)\")\n",
    "\n",
    "# Optional — class distribution proxy in each split\n",
    "def summarize_split(name, items):\n",
    "    proxy = [primary_label(lbls) for _, lbls in items]\n",
    "    c = Counter(proxy)\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    for cls, cnt in c.items():\n",
    "        print(f\"  {cls:>20s}: {cnt}\")\n",
    "\n",
    "summarize_split(\"Train\", train_items)\n",
    "summarize_split(\"Val\", val_items)\n",
    "summarize_split(\"Test\", test_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f1087",
   "metadata": {},
   "source": [
    "Cell 5: Dataset and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a7def27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — dataset & dataloaders\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "class ImageClsDataset(Dataset):\n",
    "    def __init__(self, items, tfm, name2idx):\n",
    "        self.items = items\n",
    "        self.tfm = tfm\n",
    "        self.name2idx = name2idx\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i):\n",
    "        path, cls_name = self.items[i]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = self.name2idx[cls_name]\n",
    "        return x, y\n",
    "\n",
    "train_ds = ImageClsDataset(train_items, train_tfms, name2idx)\n",
    "val_ds   = ImageClsDataset(val_items,   train_tfms, name2idx)\n",
    "test_ds  = ImageClsDataset(test_items,  train_tfms, name2idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c10cd",
   "metadata": {},
   "source": [
    "Cell 6: Model and Optimizer loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04eef622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\AppData\\Local\\Temp\\ipykernel_16096\\130186829.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type==\"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 — model & optimizer (SGD + momentum like previous version)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "def get_resnet(num_classes, variant=\"resnet18\", pretrained=True):\n",
    "    if variant == \"resnet18\":\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    elif variant == \"resnet50\":\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported variant\")\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, num_classes)\n",
    "    return m\n",
    "\n",
    "model = get_resnet(num_classes, variant=\"resnet18\", pretrained=True).to(device)\n",
    "\n",
    "# === switch back to SGD with momentum (previous config) ===\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum= MOMENTUM,           # standard momentum used before\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    nesterov=True           # optional but often beneficial\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f089dc",
   "metadata": {},
   "source": [
    "Cell 7: Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84256673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7 — train with live progress updates ===\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "THRESH = 0.5\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_acc\": [], \"train_f1\": [], \"val_f1\": []}\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, scaler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch:02d}\", leave=False)\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None and scaler.is_enabled())):\n",
    "            logits = model(xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "        if scaler and scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_match(model, loader, device, thresh=THRESH):\n",
    "    model.eval()\n",
    "    logits_list, ys = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits_list.append(model(xb).cpu())\n",
    "        ys.append(yb)\n",
    "    logits = torch.cat(logits_list)\n",
    "    probs  = logits.sigmoid().numpy()\n",
    "    y_pred = (probs >= thresh).astype(int)\n",
    "    y_true = torch.cat(ys).numpy().astype(int)\n",
    "    subset_acc = (y_pred == y_true).all(axis=1).mean()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return float(subset_acc), float(f1)\n",
    "\n",
    "best_val_acc = -1.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, device, epoch)\n",
    "    tr_acc, tr_f1   = eval_match(model, train_loader, device)\n",
    "    val_acc, val_f1 = eval_match(model, val_loader, device)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"train_f1\"].append(tr_f1)\n",
    "    history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": class_names}, CHECKPOINT_PATH)\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "        f\"Train Loss {tr_loss:.4f} | \"\n",
    "        f\"Train Acc {tr_acc:.4f} | Train F1 {tr_f1:.4f} | \"\n",
    "        f\"Val Acc {val_acc:.4f} | Val F1 {val_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "tqdm.write(f\"\\n✅ Best Val Acc: {best_val_acc:.4f} (saved to {CHECKPOINT_PATH})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774e641",
   "metadata": {},
   "source": [
    "Cell 8: Full Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618eb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 8 — evaluation (val & test) + plots ===\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_multilabel(model, loader, device, split_name=\"Val\", thresh=THRESH):\n",
    "    \"\"\"Prints multi-label metrics and returns a dict for programmatic use.\"\"\"\n",
    "    model.eval()\n",
    "    logits_list, ys = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits_list.append(model(xb).cpu())\n",
    "        ys.append(yb)\n",
    "    logits = torch.cat(logits_list)                # (N,C)\n",
    "    y_true = torch.cat(ys).numpy().astype(int)     # (N,C)\n",
    "    probs  = logits.sigmoid().numpy()\n",
    "    y_pred = (probs >= thresh).astype(int)\n",
    "\n",
    "    # Subset accuracy (exact match)\n",
    "    subset_acc = (y_pred == y_true).all(axis=1).mean()\n",
    "\n",
    "    # Macro Precision/Recall/F1\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Per-class AUC (skip classes with only one label value)\n",
    "    auc_per_class = []\n",
    "    for c in range(len(class_names)):\n",
    "        yt = y_true[:, c]\n",
    "        if len(np.unique(yt)) == 2:\n",
    "            auc_per_class.append(roc_auc_score(yt, probs[:, c]))\n",
    "        else:\n",
    "            auc_per_class.append(np.nan)\n",
    "    auc_macro = float(np.nanmean(auc_per_class))\n",
    "\n",
    "    # Specificity / FPR / G-mean per class\n",
    "    spec_list, fpr_list, rec_list, gmean_list = [], [], [], []\n",
    "    for c in range(len(class_names)):\n",
    "        yt, yp = y_true[:, c], y_pred[:, c]\n",
    "        TP = np.sum((yt==1)&(yp==1))\n",
    "        TN = np.sum((yt==0)&(yp==0))\n",
    "        FP = np.sum((yt==0)&(yp==1))\n",
    "        FN = np.sum((yt==1)&(yp==0))\n",
    "        spec = TN / (TN+FP) if (TN+FP)>0 else 0.0\n",
    "        tpr  = TP / (TP+FN) if (TP+FN)>0 else 0.0\n",
    "        fpr  = 1.0 - spec\n",
    "        gmean= np.sqrt(max(spec,0.0)*max(tpr,0.0))\n",
    "        spec_list.append(spec); fpr_list.append(fpr); rec_list.append(tpr); gmean_list.append(gmean)\n",
    "\n",
    "    spec_macro = float(np.mean(spec_list))\n",
    "    fpr_macro  = float(np.mean(fpr_list))\n",
    "    gmean_macro= float(np.mean(gmean_list))\n",
    "\n",
    "    # MCC (flattened multi-label)\n",
    "    mcc = matthews_corrcoef(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "    # Print nicely\n",
    "    print(f\"\\n=== {split_name} (threshold={thresh}) ===\")\n",
    "    print(f\"AUC-ROC (macro)      : {auc_macro:.4f}\")\n",
    "    print(f\"Subset Accuracy       : {subset_acc:.4f}\")\n",
    "    print(f\"Precision (macro)     : {prec:.4f}\")\n",
    "    print(f\"Recall (macro)        : {rec:.4f}\")\n",
    "    print(f\"F1 (macro)            : {f1:.4f}\")\n",
    "    print(f\"Specificity (macro)   : {spec_macro:.4f}\")\n",
    "    print(f\"FPR (macro)           : {fpr_macro:.4f}\")\n",
    "    print(f\"G-mean (macro)        : {gmean_macro:.4f}\")\n",
    "    print(f\"MCC (flattened)       : {mcc:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class (AUC / Spec / FPR / Recall / G-mean):\")\n",
    "    for i, cname in enumerate(class_names):\n",
    "        print(f\"{cname:20s}  \"\n",
    "              f\"AUC:{auc_per_class[i]:6.3f}  \"\n",
    "              f\"Spec:{spec_list[i]:6.3f}  \"\n",
    "              f\"FPR:{fpr_list[i]:6.3f}  \"\n",
    "              f\"Rec:{rec_list[i]:6.3f}  \"\n",
    "              f\"G:{gmean_list[i]:6.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"auc_macro\": auc_macro,\n",
    "        \"subset_acc\": float(subset_acc),\n",
    "        \"precision_macro\": float(prec),\n",
    "        \"recall_macro\": float(rec),\n",
    "        \"f1_macro\": float(f1),\n",
    "        \"specificity_macro\": spec_macro,\n",
    "        \"fpr_macro\": fpr_macro,\n",
    "        \"gmean_macro\": gmean_macro,\n",
    "        \"mcc\": float(mcc),\n",
    "        \"auc_per_class\": auc_per_class,\n",
    "    }\n",
    "\n",
    "# --- Run evaluation on validation and test ---\n",
    "val_metrics  = evaluate_multilabel(model, val_loader,  device, \"Val\",  THRESH)\n",
    "test_metrics = evaluate_multilabel(model, test_loader, device, \"Test\", THRESH)\n",
    "\n",
    "# --- Plot training curves from Cell 7 history ---\n",
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_loss\"], marker='o')\n",
    "plt.title(\"Train Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_acc\"], marker='o', label=\"Train Acc (subset)\")\n",
    "plt.plot(epochs, history[\"val_acc\"],   marker='o', label=\"Val Acc (subset)\")\n",
    "plt.title(\"Subset Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_f1\"], marker='o', label=\"Train F1 (macro)\")\n",
    "plt.plot(epochs, history[\"val_f1\"],   marker='o', label=\"Val F1 (macro)\")\n",
    "plt.title(\"F1 (macro) per Epoch\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"F1\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
